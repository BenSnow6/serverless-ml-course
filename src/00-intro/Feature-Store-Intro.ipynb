{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076a7f63",
   "metadata": {},
   "source": [
    "## Feature Store Basics\n",
    "\n",
    "This notebook introduces the basics of working with the Hopsworks API and Pandas DataFrames.\n",
    "\n",
    "First, we will define a Pandas DataFrame with 4 credit card transactions in 3 different cities with the same credit card. The last 2 credit card transactions are labeled as 'fraud', while the first 2 transactions are labeled as 'not fraud'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9411b6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_card_number</th>\n",
       "      <th>trans_datetime</th>\n",
       "      <th>amount</th>\n",
       "      <th>location</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-01 08:44:00</td>\n",
       "      <td>142.34</td>\n",
       "      <td>Sao Paolo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 19:44:00</td>\n",
       "      <td>12.34</td>\n",
       "      <td>Rio De Janeiro</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 20:44:00</td>\n",
       "      <td>66.29</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 20:55:00</td>\n",
       "      <td>112.33</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    credit_card_number      trans_datetime  amount        location  fraud\n",
       "0  1111 2222 3333 4444 2022-01-01 08:44:00  142.34       Sao Paolo  False\n",
       "1  1111 2222 3333 4444 2022-01-02 19:44:00   12.34  Rio De Janeiro  False\n",
       "2  1111 2222 3333 4444 2022-01-02 20:44:00   66.29       Stockholm   True\n",
       "3  1111 2222 3333 4444 2022-01-02 20:55:00  112.33       Stockholm   True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { \n",
    "    'credit_card_number': ['1111 2222 3333 4444', '1111 2222 3333 4444','1111 2222 3333 4444',\n",
    "                           '1111 2222 3333 4444'],\n",
    "    'trans_datetime': ['2022-01-01 08:44', '2022-01-02 19:44', '2022-01-02 20:44', '2022-01-02 20:55'],\n",
    "    'amount': [142.34, 12.34, 66.29, 112.33],\n",
    "    'location': ['Sao Paolo', 'Rio De Janeiro', 'Stockholm', 'Stockholm'],\n",
    "    'fraud': [False, False, True, True] \n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['trans_datetime']= pd.to_datetime(df['trans_datetime'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e9f2d",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks\n",
    "\n",
    "You need an API key to connect. First, login to Hopsworks, then run this code. It will provide a link to get your API key, that you then need to copy and paste into the text box that appears below this cell.\n",
    "\n",
    "It is good practice to save this API key somewhere safe so you don't have to create a new one every time you use Hopsworks. If you run this code on your laptop, a copy of the API key will be cached locally in this directory in a file with restricted permissions, so you don't have to always re-enter the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a705ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks\n",
      "  Downloading hopsworks-3.0.3.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hsfs[python]<3.1.0,>=3.0.0\n",
      "  Downloading hsfs-3.0.4.tar.gz (120 kB)\n",
      "     -------------------------------------- 120.5/120.5 kB 6.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hsml<3.1.0,>=3.0.0\n",
      "  Downloading hsml-3.0.2.tar.gz (51 kB)\n",
      "     ---------------------------------------- 51.1/51.1 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyhumps==1.6.1\n",
      "  Downloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hopsworks) (2.27.1)\n",
      "Collecting furl\n",
      "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: boto3 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hopsworks) (1.21.21)\n",
      "Collecting pyjks\n",
      "  Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting mock\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tqdm in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hopsworks) (4.63.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.24.4 requires botocore==1.26.4, but you have botocore 1.24.46 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.3.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.21.6)\n",
      "Collecting avro==1.10.2\n",
      "  Downloading avro-1.10.2.tar.gz (68 kB)\n",
      "     ---------------------------------------- 68.2/68.2 kB 3.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.41-cp37-cp37m-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.3 MB/s eta 0:00:00\n",
      "Collecting PyMySQL[rsa]\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 43.8/43.8 kB ? eta 0:00:00\n",
      "Collecting great_expectations==0.14.12\n",
      "  Downloading great_expectations-0.14.12-py3-none-any.whl (4.9 MB)\n",
      "     ---------------------------------------- 4.9/4.9 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markupsafe<2.1.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.0.1)\n",
      "Collecting pyhopshive[thrift]\n",
      "  Downloading PyHopsHive-0.6.4.1.dev0.tar.gz (42 kB)\n",
      "     ---------------------------------------- 42.9/42.9 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pyarrow in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.0.0)\n",
      "Collecting confluent-kafka==1.8.2\n",
      "  Downloading confluent_kafka-1.8.2-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting fastavro==1.4.11\n",
      "  Downloading fastavro-1.4.11-cp37-cp37m-win_amd64.whl (402 kB)\n",
      "     ------------------------------------- 402.9/402.9 kB 12.7 MB/s eta 0:00:00\n",
      "Collecting pyparsing<3,>=2.4\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: cryptography>=3.2 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (36.0.1)\n",
      "Requirement already satisfied: pytz>=2021.3 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2021.3)\n",
      "Requirement already satisfied: nbformat>=5.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.1.3)\n",
      "Requirement already satisfied: colorama>=0.4.3 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.11.2)\n",
      "Requirement already satisfied: Click>=7.1.2 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (8.0.4)\n",
      "Requirement already satisfied: jinja2<3.1.0,>=2.10 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.0.3)\n",
      "Collecting ruamel.yaml<0.17.18,>=0.16\n",
      "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl (109 kB)\n",
      "     -------------------------------------- 109.1/109.1 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.1.1)\n",
      "Collecting jsonpatch>=1.22\n",
      "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.26.8)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.2)\n",
      "Requirement already satisfied: mistune<2.0.0,>=0.8.4 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.8.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (21.3)\n",
      "Requirement already satisfied: altair<5,>=4.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.1.0)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.6.5)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.6.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from requests->hopsworks) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from requests->hopsworks) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from requests->hopsworks) (3.3)\n",
      "Collecting botocore<1.25.0,>=1.24.21\n",
      "  Downloading botocore-1.24.46-py3-none-any.whl (8.7 MB)\n",
      "     ---------------------------------------- 8.7/8.7 MB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from boto3->hopsworks) (0.5.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from boto3->hopsworks) (1.0.0)\n",
      "Collecting orderedmultidict>=1.0.1\n",
      "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.8.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from furl->hopsworks) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from pyjks->hopsworks) (0.4.8)\n",
      "Collecting pycryptodomex\n",
      "  Downloading pycryptodomex-3.15.0-cp35-abi3-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 8.2 MB/s eta 0:00:00\n",
      "Collecting twofish\n",
      "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from pyjks->hopsworks) (0.2.8)\n",
      "Collecting javaobj-py3\n",
      "  Downloading javaobj_py3-0.4.3-py2.py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.3/57.3 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: future in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from pyhopshive[thrift]->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.18.2)\n",
      "Collecting thrift>=0.10.0\n",
      "  Downloading thrift-0.16.0.tar.gz (59 kB)\n",
      "     ---------------------------------------- 59.6/59.6 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.3-cp37-cp37m-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 101.3/101.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: toolz in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.11.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from cryptography>=3.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from importlib-metadata>=1.7.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.7.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.20.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.5.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.5.5)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.0.2)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.4.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbformat>=5.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.9.2)\n",
      "Collecting ruamel.yaml.clib>=0.1.2\n",
      "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-win_amd64.whl (115 kB)\n",
      "     -------------------------------------- 115.6/115.6 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: backports.zoneinfo in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from tzlocal>=1.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from tzlocal>=1.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2022.4)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from tzlocal>=1.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.1.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.21)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (7.1.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (3.0.27)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (59.8.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.11.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (6.4.8)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from jupyter-core->nbformat>=5.0->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (303)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.13.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.13.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (21.3.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.5.4)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.8.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (6.4.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (21.2.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.5.11)\n",
      "Requirement already satisfied: bleach in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (4.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\bensn\\anaconda3\\envs\\tf1\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]<3.1.0,>=3.0.0->hopsworks) (0.5.1)\n",
      "Building wheels for collected packages: hopsworks, avro, hsml, hsfs, twofish, thrift, pyhopshive\n",
      "  Building wheel for hopsworks (setup.py): started\n",
      "  Building wheel for hopsworks (setup.py): finished with status 'done'\n",
      "  Created wheel for hopsworks: filename=hopsworks-3.0.3-py3-none-any.whl size=64178 sha256=30540994e0cc4bffbbd983f08742d69f3ec7c97fef51b5d121dd3b6a8ab7496b\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\65\\f5\\8b\\3c6773f5c1dfcb508f749c1235b3262fcd62c088f2f188486f\n",
      "  Building wheel for avro (setup.py): started\n",
      "  Building wheel for avro (setup.py): finished with status 'done'\n",
      "  Created wheel for avro: filename=avro-1.10.2-py3-none-any.whl size=96832 sha256=902487da842b4bdf6e4f5901e3e0026ea8ac1b4ae33e4cea1faf247e140aade5\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\e7\\93\\e8\\7e16388beb0837cbfb9065ff9d3fe33e4111a3f4bedea1c2c6\n",
      "  Building wheel for hsml (setup.py): started\n",
      "  Building wheel for hsml (setup.py): finished with status 'done'\n",
      "  Created wheel for hsml: filename=hsml-3.0.2-py3-none-any.whl size=99263 sha256=dbaec76ffb649bf6a1a923370fed5133dbf5cac24ea99ad634a011ad535ef13a\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\b0\\c1\\d6\\b1561091cb1b9a783d5640def9eb22c6bd7078c0429f9f46e8\n",
      "  Building wheel for hsfs (setup.py): started\n",
      "  Building wheel for hsfs (setup.py): finished with status 'done'\n",
      "  Created wheel for hsfs: filename=hsfs-3.0.4-py3-none-any.whl size=175790 sha256=a4081aa3d178fe50d9cc6e3cb60d7e7e9c789493ab11bea0a84d42cd1b565f01\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\79\\48\\61\\94dd019e5cba0578c3819fc6ccf7510b54bc3ccba164a0c40e\n",
      "  Building wheel for twofish (setup.py): started\n",
      "  Building wheel for twofish (setup.py): finished with status 'done'\n",
      "  Created wheel for twofish: filename=twofish-0.3.0-cp37-cp37m-win_amd64.whl size=13800 sha256=cd3bc9470d641c6b7375291955132b3cab5340e7e9d7764dbb0681a6cdda7b26\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\ae\\fd\\45\\5f33a6da0bf1f7643ed2b7f4ef124d6fe049d2b1604ccdf83d\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.16.0-cp37-cp37m-win_amd64.whl size=176197 sha256=679a68e4ebde523224e748b28d3a6426a9f3b40a9cec4aab34dcf4f897972396\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\e6\\d0\\8c\\11a1840ef59493f9221d13748a29482323f13f6da6b236c3de\n",
      "  Building wheel for pyhopshive (setup.py): started\n",
      "  Building wheel for pyhopshive (setup.py): finished with status 'done'\n",
      "  Created wheel for pyhopshive: filename=PyHopsHive-0.6.4.1.dev0-py3-none-any.whl size=48587 sha256=74c0e1dbbc7d0f952ed4e73a16036e8c00734bc475eeed16736b98b74dbf122c\n",
      "  Stored in directory: c:\\users\\bensn\\appdata\\local\\pip\\cache\\wheels\\2b\\b4\\42\\f8b3ec9ab77d90b9d3e463000a8b85fbeb01e6dc41663cd5d6\n",
      "Successfully built hopsworks avro hsml hsfs twofish thrift pyhopshive\n",
      "Installing collected packages: twofish, pyhumps, javaobj-py3, confluent-kafka, thrift, ruamel.yaml.clib, pyparsing, PyMySQL, pycryptodomex, orderedmultidict, mock, jsonpointer, greenlet, fastavro, avro, sqlalchemy, ruamel.yaml, pyjks, jsonpatch, furl, botocore, pyhopshive, hsml, great_expectations, hsfs, hopsworks\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.7\n",
      "    Uninstalling pyparsing-3.0.7:\n",
      "      Successfully uninstalled pyparsing-3.0.7\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.26.4\n",
      "    Uninstalling botocore-1.26.4:\n",
      "      Successfully uninstalled botocore-1.26.4\n",
      "Successfully installed PyMySQL-1.0.2 avro-1.10.2 botocore-1.24.46 confluent-kafka-1.8.2 fastavro-1.4.11 furl-2.1.3 great_expectations-0.14.12 greenlet-1.1.3 hopsworks-3.0.3 hsfs-3.0.4 hsml-3.0.2 javaobj-py3-0.4.3 jsonpatch-1.32 jsonpointer-2.3 mock-4.0.3 orderedmultidict-1.0.1 pycryptodomex-3.15.0 pyhopshive-0.6.4.1.dev0 pyhumps-1.6.1 pyjks-20.0.0 pyparsing-2.4.7 ruamel.yaml-0.17.17 ruamel.yaml.clib-0.2.6 sqlalchemy-1.4.41 thrift-0.16.0 twofish-0.3.0\n"
     ]
    }
   ],
   "source": [
    "%pip install hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c855e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy your Api Key (first register/login): https://c.app.hopsworks.ai/account/api/generated\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/2289\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "proj = hopsworks.login()\n",
    "fs = proj.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d38175",
   "metadata": {},
   "source": [
    "### Create a Feature Group\n",
    "\n",
    "A feature group is a table of features that are computed together in the same feature pipeline and written as a DataFrame to the Feature Store. You should have a unique idenitfier for each row that may be one or more columns, and you define as the `primary_key`. You may also have a column that represents the timestamp or datetime for when row values were observed. If so, you should specify the `event_time` column when creating the Feature Group.\n",
    "\n",
    "Hopsworks have comprehensive documentation on Feature Groups. Click on these links to learn more.\n",
    "\n",
    "* [Feature Group Concept](https://docs.hopsworks.ai/3.0/concepts/fs/feature_group/fg_overview/)\n",
    "* [Feature Group Creation Guide](https://docs.hopsworks.ai/3.0/user_guides/fs/feature_group/create/)\n",
    "* [Feature Group API Docs](https://docs.hopsworks.ai/feature-store-api/3.0/generated/api/feature_group_api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea7a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_transactions\",\n",
    "     version=1,\n",
    "     description=\"Credit Card Transaction data\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='trans_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c0972",
   "metadata": {},
   "source": [
    "### Write your DataFrame to the Feature Group\n",
    "When you write your DataFrame to the feature group, first the DataFrame is copied to Hopsworks. \n",
    "Then a backfill ingestion job is run on Hopsworks to insert/append the DataFrame to the Feature Group. \n",
    "The job is a Spark job, and the data is stored in a Apache Hudi table in Hopsworks.\n",
    "\n",
    "It will take about 1 minute for the ingestion job to complete.\n",
    "If you don't want to wait 1 minute, you make the ingestion job run in the background with:\n",
    "\n",
    "\n",
    "    fg.insert(df, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3380610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/2289/fs/2234/fg/2883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 4/4 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/2289/jobs/named/credit_card_transactions_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x15349d63d08>, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg.insert(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2bc45",
   "metadata": {},
   "source": [
    "## Read using Feature Views\n",
    "\n",
    "When you want to use features to train or serve models, you create Feature that are `labels` View a Feature View by first selecting features from Feature Groups. Here, we only have 1 Feature Group, and we select 3 features from it, returning a `query` object. The `query` object defines the set of features (or schema) for a Feature View. \n",
    "\n",
    "You create a Feature View with a `query` object (specifying the features and any extra columns that might be needed for inference (but not training)), providing a name and version, and specifying the columns that are `labels`, that is, the target your machine learning algorithm will try and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7e3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/2289/fs/2234/fv/credit_card_transactions/version/1\n"
     ]
    }
   ],
   "source": [
    "query = fg.select([\"amount\", \"location\", \"fraud\"])\n",
    "\n",
    "fv = fs.create_feature_view(name=\"credit_card_transactions\",\n",
    "                            version=1,\n",
    "                            description=\"Features from the credit_card_transactions FG\",\n",
    "                            labels=[\"fraud\"],\n",
    "                            query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b2099",
   "metadata": {},
   "source": [
    "### Splitting into Train/Test sets\n",
    "\n",
    "With a Feature View, you can read train and test sets directly as Pandas DataFrames - similar to scikit-learn.\n",
    "Here, \n",
    "\n",
    "* `X_train` is the features of our train set, \n",
    "* `y_train` is the labels of our train set, \n",
    "* `X_test` is the features of our test set, \n",
    "* `y_test` is the labels of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792da473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = fv.train_test_split(0.5)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef54b2",
   "metadata": {},
   "source": [
    "### Saving training data as files\n",
    "Sometimes, if you have a large volume of training data, it is better to save training data as files. Then read the files in your training pipeline. You can create training data as CSV files that is randomly split into train/test sets as follows (the `td_version` is the version of the training data for this feature view, and you can track the progress of the job used to create the training data using the `td_job` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578e424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/2289/jobs/named/credit_card_transactions_1_1_create_fv_td_05102022114302/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `1`.\n"
     ]
    }
   ],
   "source": [
    "td_version, td_job = fv.create_train_test_split(\n",
    "    description = 'Transactions fraud batch training dataset',\n",
    "    data_format = 'csv',\n",
    "    test_size = 0.5,\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b958f",
   "metadata": {},
   "source": [
    "## Training Data as files\n",
    "The training data is now stored as a CSV file on Hopsworks under `Project Settings` -> `File Browser` -> <username>_Training_Datasets.\n",
    "    \n",
    "You can read the training data as split train/test sets with the following. Note the parameter `td_version` we pass here. A feature view can have many training datasets, so you need to supply the version you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e53364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fv.get_train_test_split(td_version)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6d416",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "Compute the total amount spent on the credit card by first grouping all the rows together with the same `credit_card_number` and then summing up their amounts. \n",
    "\n",
    "The code first creates a new DataFrame with only the `credit_card_number` and `amount` columns, then the logic of a group-by could be described as \n",
    "\n",
    "    for-each (`credit_card_number`) do \\sigma amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"credit_card_number\", \"amount\"]].groupby(\"credit_card_number\").sum()\n",
    "df2.rename(columns={\"amount\": \"total_spent\"}, inplace=True)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187468e",
   "metadata": {},
   "source": [
    " We might also want to know at what point-in-time was that total and add a column with the datetime of the last (most recent) credit card transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04244e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"as_of_datetime\"] = df[[\"credit_card_number\", \"trans_datetime\"]].groupby(\"credit_card_number\").max()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27a872",
   "metadata": {},
   "source": [
    "The `groupby` operation sets `credit_card_number` as the index of our DataFrame.\n",
    "We want `credit_card_number` as a column, as Pandas indexes are not written to the Feature Group.\n",
    "We can move the index to a column using `reset_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8253595",
   "metadata": {},
   "source": [
    "We create a feature group to store the contents of `df2` with our aggregated credit card spending information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg2 = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_spending\",\n",
    "     version=1,\n",
    "     description=\"Credit Card Spending\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='as_of_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg2.insert(df2, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7aedf",
   "metadata": {},
   "source": [
    "Let's add some more data to our original feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = { \n",
    "    'credit_card_number': ['9999 8888 7777 6666', '9999 8888 7777 6666','9999 8888 7777 6666',\n",
    "                           '9999 8888 7777 6666'],\n",
    "    'trans_datetime': ['2022-01-02 04:11', '2022-01-03 07:24', '2022-01-05 10:33', '2022-01-05 11:50'],\n",
    "    'amount': [55.67, 84, 77.95, 183],\n",
    "    'location': ['San Francisco', 'San Francisco', 'Dublin', 'Dublin'],\n",
    "    'fraud': [False, False, False, False] \n",
    "}\n",
    "\n",
    "df3 = pd.DataFrame.from_dict(more_data)\n",
    "df3['trans_datetime']= pd.to_datetime(df3['trans_datetime'])\n",
    "\n",
    "fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "\n",
    "fg.insert(df3, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1d883",
   "metadata": {},
   "source": [
    "Now let's compute how much money was spent on the card since the last time we computed amount spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2dc55",
   "metadata": {},
   "source": [
    "## Time Series: Window Aggregations\n",
    "\n",
    "Count the amount of money spent per day (make the length of the window '1d').\n",
    "We will need to set the `event_time` column as the index in order to use Pandas built-in window aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = fg.read()\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.set_index('trans_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b325eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df5['rolling_max_1d'] = df5.rolling('1D').amount.max()\n",
    "    df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['rolling_mean_1d'] = df5.rolling('1D').amount.mean()\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_agg = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_rolling_windows\",\n",
    "     version=1,\n",
    "     description=\"Daily Credit Card Spending\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='trans_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b05b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_agg.insert(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31174a83",
   "metadata": {},
   "source": [
    "### Create a Feature View using features from multiple Feature Groups\n",
    "\n",
    "We want to create a model that uses features from multiple feature groups. \n",
    "We will select features from the different feature groups and join them together to create a query object. \n",
    "We can read the data in the query object as a DataFrame to inspect it before we create the feature view. \n",
    "We will use the feature view to read the training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf25f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = fg.select_all().join(fg_agg.select(['rolling_max_1d', 'rolling_mean_1d']))\n",
    "\n",
    "training_data = query.read()\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = fs.create_feature_view(name=\"credit_card_fraud_rolling\",\n",
    "                            description=\"Features for a model to predict credit card fraud, including rolling windows\",\n",
    "                            version=1,\n",
    "                            query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fv.train_test_split(0.5)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7576002",
   "metadata": {},
   "source": [
    "### Read from Feature Groups\n",
    "\n",
    "You are also able to read data from Feature Groups as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1717fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "read_df = fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67029b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5aa606",
   "metadata": {},
   "source": [
    "### Filters\n",
    "You can use filters on the `query` object or on the Feature Groups, when reading from them. Here, we read all rows where the transaction amount is greater than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.feature import Feature\n",
    "\n",
    "big_amounts_df = fg.filter(Feature(\"amount\") > 100).read()\n",
    "big_amounts_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tf1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "48324878fd09c2ff1755460c6bedcd7035239e95f1ec0df88470a6ba78f59e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
